@article{shirai2024open,
  title={オープンドメイン手順書のフローグラフ予測},
  author={白井 圭佑 and 亀甲 博貴 and 森 信介},
  journal={自然言語処理},
  volume={31},
  number={2},
  pages={479-503},
  year={2024},
  doi={10.5715/jnlp.31.479}
}

@article{nishimura2024recipe,
  author = {Nishimura, Taichi and Hashimoto, Atsushi and Ushiku, Yoshitaka and Kameko, Hirotaka and Mori, Shinsuke},
  title = {Recipe Generation from Unsegmented Cooking Videos},
  year = 2024,
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1551-6857},
  url = {https://doi.org/10.1145/3649137},
  doi = {10.1145/3649137},
  abstract = {This paper tackles recipe generation from unsegmented cooking videos, a task that requires agents to (1) extract key events in completing the dish and (2) generate sentences for the extracted events. Our task is similar to dense video captioning (DVC), which aims at detecting events thoroughly and generating sentences for them. However, unlike DVC, in recipe generation, recipe story awareness is crucial, and a model should extract an appropriate number of events in the correct order and generate accurate sentences based on them. We analyze the output of the DVC model and confirm that although (1) several events are adoptable as a recipe story, (2) the generated sentences for such events are not grounded in the visual content. Based on this, we set our goal to obtain correct recipes by selecting oracle events from the output events and re-generating sentences for them. To achieve this, we propose a transformer-based multimodal recurrent approach of training an event selector and sentence generator for selecting oracle events from the DVC’s events and generating sentences for them. In addition, we extend the model by including ingredients to generate more accurate recipes. The experimental results show that the proposed method outperforms state-of-the-art DVC models. We also confirm that, by modeling the recipe in a story-aware manner, the proposed model outputs the appropriate number of events in the correct order.},
  note = {Just Accepted},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  month = feb,
  keywords = {cooking recipe, video understanding}
}

@article{shirai2023visual,
  title={調理動作後の物体の視覚的状態予測を目指した Visual Recipe Flow データセットの構築と評価},
  author={白井 圭佑 and 橋本 敦史 and 西村 太一 and 亀甲 博貴 and 栗田 修平 and 森 信介},
  journal={自然言語処理},
  volume={30},
  number={3},
  pages={1042-1060},
  year=2023,
  doi={10.5715/jnlp.30.1042}
}

@article{kameko2023japanese,
  author={Kameko, Hirotaka and Murawaki, Yugo and Matsuyoshi, Suguru and Mori, Shinsuke},
  journal={IEEE Access}, 
  title={Japanese Event Factuality Analysis in the Era of BERT}, 
  year=2023,
  volume={11},
  number={},
  pages={93286-93292},
  keywords={Task analysis;Multitasking;Tagging;Annotations;Training data;Online services;Neural networks;Event detection;Labeling;Sequential analysis;Event factuality;modality;sequence labeling;neural networks;multi-task learning},
  doi={10.1109/ACCESS.2023.3308916}
}

@article{nishimura2023state,
  author = {Nishimura, Taichi and Hashimoto, Atsushi and Ushiku, Yoshitaka and Kameko, Hirotaka and Mori, Shinsuke},
  title = {State-aware video procedural captioning},
  year = {2023},
  issue_date = {Oct 2023},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {82},
  number = {24},
  issn = {1380-7501},
  url = {https://doi.org/10.1007/s11042-023-14774-7},
  doi = {10.1007/s11042-023-14774-7},
  abstract = {Video procedural captioning (VPC), which generates procedural text from instructional videos, is an essential task for scene understanding and real-world applications. The main challenge of VPC is to describe how to manipulate materials accurately. This paper focuses on this challenge by designing a new VPC task, generating a procedural text from the clip sequence of an instructional video and material set. In this task, the state of materials is sequentially changed by manipulations, yielding their state-aware visual representations (e.g., eggs are transformed into cracked, stirred, then fried forms). The essential difficulty is to convert such visual representations into textual representations; that is, a model should track the material states after manipulations to better associate the cross-modal relations. To achieve this, we propose a novel VPC method, which modifies an existing textual simulator for tracking material states as a visual simulator and incorporates it into a video captioning model. Our experimental results show the effectiveness of the proposed method, which outperforms state-of-the-art video captioning models. We further analyze the learned embedding of materials to demonstrate that the simulators capture their state transition.},
  journal = {Multimedia Tools Appl.},
  month = {mar},
  pages = {37273–37301},
  numpages = {29},
  keywords = {Instructional video, Procedural text, Simulator}
}

@article{nishimura2022biovl2,
  title={BioVL2データセット：生化学分野における一人称視点の実験映像への言語アノテーション},
  author={西村 太一 and 迫田 航次郎 and 牛久 敦 and 橋本 敦史 and 奥田 奈津子 and 小野 富三人 and 亀甲 博貴 and 森 信介},
  journal={自然言語処理},
  volume={29},
  number={4},
  pages={1106-1137},
  year=2022,
  doi={10.5715/jnlp.29.1106},
  memo={論文賞受賞}
}

@article{kameko2021japanese,
  title={将棋解説文への固有表現・モダリティ情報アノテーション},
  author={亀甲 博貴 and 松吉 俊 and John Richardson and 牛久 敦 and 笹田 鉄郎 and 村脇 有吾 and 鶴岡 慶雅 and 森 信介},
  journal={自然言語処理},
  volume={28},
  number={3},
  pages={847-873},
  year=2021,
  doi={10.5715/jnlp.28.847}
}

@article{nishimura2021structure,
  author={Nishimura, Taichi and Hashimoto, Atsushi and Ushiku, Yoshitaka and Kameko, Hirotaka and Yamakata, Yoko and Mori, Shinsuke},
  journal={IEEE Access}, 
  title={Structure-Aware Procedural Text Generation From an Image Sequence}, 
  year=2021,
  volume={9},
  number={},
  pages={2125-2141},
  keywords={Image sequences;Merging;Videos;Task analysis;Visualization;Oils;Annotations;Natural language processing;text generation;procedural text;vision and language},
  doi={10.1109/ACCESS.2020.3043452}
}

@article{mizukami2019deep,
   author	 = "水上 直紀 and 鈴木 潤 and 亀甲 博貴 and 鶴岡 慶雅",
   title	 = "報酬が疎な環境に適した深層強化学習法",
   journal	 = "情報処理学会論文誌",
   year 	 = 2019,
   volume	 = "60",
   number	 = "3",
   pages	 = "956--966",
   month	 = mar,
}

@article{kameko2017predicting,
   author	 = "亀甲 博貴 and 森 信介 and 鶴岡 慶雅",
   title	 = "将棋解説文生成のための解説すべき手順の予測",
   journal	 = "情報処理学会論文誌",
   year 	 = 2017,
   volume	 = "58",
   number	 = "12",
   pages	 = "2070--2079",
   month	 = dec,
}

@article{tomori2017improvement,
  title={シンボルグラウンディングによる分野特有の単語分割の精度向上},
  author={友利 涼 and 亀甲 博貴 and 二宮 崇 and 森 信介 and 鶴岡 慶雅},
  journal={自然言語処理},
  volume={24},
  number={3},
  pages={447-461},
  year=2017,
  doi={10.5715/jnlp.24.447}
}

@article{kameko2014automatic,
   author	 = "亀甲 博貴 and 三輪 誠 and 鶴岡 慶雅 and 森 信介 and 近山 隆",
   title	 = "対数線形言語モデルを用いた将棋解説文の自動生成",
   journal	 = "情報処理学会論文誌",
   year 	 = 2014,
   volume	 = "55",
   number	 = "11",
   pages	 = "2431--2440",
   month	 = nov,
}

@inproceedings{nishimoto2025biovl-qr,
  author={Nishimoto, Tomohiro and Nishimura, Taichi and Yamamoto, Koki and Shirai, Keisuke and Kameko, Hirotaka and Haneji, Yuto and Yoshida, Tomoya and Kajimura, Keiya and Cui, Taiyu and Nishiwaki, Chihiro and Daikoku, Eriko and Okuda, Natsuko and Ono, Fumihito and Mori, Shinsuke},
  booktitle={2025 IEEE International Conference on Image Processing (ICIP)}, 
  title={BioVL-QR: Egocentric Biochemical Vision-And-Language Dataset Using Micro QR Codes}, 
  year={2025},
  volume={},
  number={},
  pages={695-700},
  keywords={Location awareness;Hands;Protocols;Annotations;Image processing;QR codes;Detectors;Manuals;Labeling;Videos;Vision-and-language;Egocentric Vision;Biochemistry;Micro QR Codes},
  doi={10.1109/ICIP55913.2025.11084416},
  url={https://doi.org/10.1109/ICIP55913.2025.11084416}
}

@inproceedings{nakata2024texylon,
  author={Nakata, Masato and Morita, Kosuke and Kameko, Hirotaka and Mori, Shinsuke},
  title={Texylon: Dataset of Log-to-Description and Description-to-Log Generation for Text Analytics Tools},
  booktitle={the 8th International Workshop on SCIentific DOCument Analysis (SCIDOCA 2024)},
  month=may,
  year=2024,
}

@inproceedings{ohno2024automatic,
    title = "Automatic Construction of a Large-Scale Corpus for Geoparsing Using {W}ikipedia Hyperlinks",
    author = "Ohno, Keyaki  and
      Kameko, Hirotaka  and
      Shirai, Keisuke  and
      Nishimura, Taichi  and
      Mori, Shinsuke",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = 2024,
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.168",
    pages = "1883--1888",
    abstract = "Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts. Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation. For evaluating geoparsing systems, several corpora have been proposed in previous work. However, these corpora are small-scale and suffer from the coverage of location expressions on general domains. In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates. With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions. 45.6{\%} of location expressions are ambiguous and refer to more than one location with the same notation. In each article, location expressions of the article title and those hyperlinks to other articles are assigned with coordinates. By utilizing hyperlinks, we can accurately assign location expressions with coordinates even with ambiguous location expressions in the texts. Experimental results show that there remains room for improvement by disambiguating location expressions.",
}

@inproceedings{shirai2023towards,
    title = "Towards Flow Graph Prediction of Open-Domain Procedural Texts",
    author = "Shirai, Keisuke  and
      Kameko, Hirotaka  and
      Mori, Shinsuke",
    editor = "Can, Burcu  and
      Mozes, Maximilian  and
      Cahyawijaya, Samuel  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Ravfogel, Shauli  and
      Ravichander, Abhilasha  and
      Zhao, Chen  and
      Augenstein, Isabelle  and
      Rogers, Anna  and
      Cho, Kyunghyun  and
      Grefenstette, Edward  and
      Voita, Lena",
    booktitle = "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
    month = jul,
    year = 2023,
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.repl4nlp-1.8",
    doi = "10.18653/v1/2023.repl4nlp-1.8",
    pages = "87--96",
    abstract = "Machine comprehension of procedural texts is essential for reasoning about the steps and automating the procedures. However, this requires identifying entities within a text and resolving the relationships between the entities. Previous work focused on the cooking domain and proposed a framework to convert a recipe text into a flow graph (FG) representation. In this work, we propose a framework based on the recipe FG for flow graph prediction of open-domain procedural texts. To investigate flow graph prediction performance in non-cooking domains, we introduce the wikiHow-FG corpus from articles on wikiHow, a website of how-to instruction articles. In experiments, we consider using the existing recipe corpus and performing domain adaptation from the cooking to the target domain. Experimental results show that the domain adaptation models achieve higher performance than those trained only on the cooking or target domain data.",
}

@inproceedings{shirai2022visual,
    title = "Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows",
    author = "Shirai, Keisuke  and
      Hashimoto, Atsushi  and
      Nishimura, Taichi  and
      Kameko, Hirotaka  and
      Kurita, Shuhei  and
      Ushiku, Yoshitaka  and
      Mori, Shinsuke",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = 2022,
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.315",
    pages = "3570--3577",
    abstract = "We present a new multimodal dataset called Visual Recipe Flow, which enables us to learn a cooking action result for each object in a recipe text. The dataset consists of object state changes and the workflow of the recipe text. The state change is represented as an image pair, while the workflow is represented as a recipe flow graph. We developed a web interface to reduce human annotation costs. The dataset allows us to try various applications, including multimodal information retrieval.",
}

@inproceedings{tanaka2022image,
    title = "Image Description Dataset for Language Learners",
    author = "Tanaka, Kento  and
      Nishimura, Taichi  and
      Nanjo, Hiroaki  and
      Shirai, Keisuke  and
      Kameko, Hirotaka  and
      Dantsuji, Masatake",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = 2022,
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.735",
    pages = "6814--6821",
    abstract = "We focus on image description and a corresponding assessment system for language learners. To achieve automatic assessment of image description, we construct a novel dataset, the Language Learner Image Description (LLID) dataset, which consists of images, their descriptions, and assessment annotations. Then, we propose a novel task of automatic error correction for image description, and we develop a baseline model that encodes multimodal information from a learner sentence with an image and accurately decodes a corrected sentence. Our experimental results show that the developed model can revise errors that cannot be revised without an image.",
}

@inproceedings{sung2021inference,
  author={Sung, Junehwan and Mori, Shinsuke and Kameko, Hirotaka and Kubo, Akira and Sekino, Tatsuki},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={Inference of Absolute Time Value from Temporal Expressions}, 
  year=2021,
  volume={},
  number={},
  pages={2273-2280},
  keywords={Training;Text recognition;Conferences;Natural languages;Manuals;Big Data;Inference algorithms;temporal expression;absolute time value;named entity recognition},
  doi={10.1109/BigData52589.2021.9671863}
}

@inproceedings{nishimura2021state,
  author = {Nishimura, Taichi and Hashimoto, Atsushi and Ushiku, Yoshitaka and Kameko, Hirotaka and Mori, Shinsuke},
  title = {State-aware Video Procedural Captioning},
  year = 2021,
  isbn = {9781450386517},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3474085.3475322},
  doi = {10.1145/3474085.3475322},
  abstract = {Video procedural captioning (VPC), which generates procedural text from instructional videos, is an essential task for scene understanding and real-world applications. The main challenge of VPC is to describe how to manipulate materials accurately. This paper focuses on this challenge by designing a new VPC task, generating a procedural text from the clip sequence of an instructional video and material list. In this task, the state of materials is sequentially changed by manipulations, yielding their state-aware visual representations (e.g., eggs are transformed into cracked, stirred, then fried forms). The essential difficulty is to convert such visual representations into textual representations; that is, a model should track the material states after manipulations to better associate the cross-modal relations. To achieve this, we propose a novel VPC method, which modifies an existing textual simulator for tracking material states as a visual simulator and incorporates it into a video captioning model. Our experimental results show the effectiveness of the proposed method, which outperforms state-of-the-art video captioning models. We further analyze the learned embedding of materials to demonstrate that the simulators capture their state transition. The code and dataset are available from https://github.com/misogil0116/svpc},
  booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages = {1766-1774},
  numpages = {9},
  keywords = {simulator, procedural text, instructional video},
  location = {Virtual Event, China},
  series = {MM '21}
}

@inproceedings{nishimura2021egocentric,
    author    = {Nishimura, Taichi and Sakoda, Kojiro and Hashimoto, Atsushi and Ushiku, Yoshitaka and Tanaka, Natsuko and Ono, Fumihito and Kameko, Hirotaka and Mori, Shinsuke},
    title     = {Egocentric Biochemical Video-and-Language Dataset},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = oct,
    year      = 2021,
    pages     = {3129-3133}
}

@inproceedings{hara2021development,
  author={Hara, Shoichiro and Kubo, Akira and Matsuzaki, Masato and Kameko, Hirotaka and Mori, Shinsuke},
  booktitle={2021 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC)}, 
  title={Development of Methods to Extract Place Names and Estimate Their Places from Web Newspaper Articles}, 
  year=2021,
  volume={},
  number={},
  pages={1-6},
  keywords={Social sciences;Meetings;Focusing;Media;Big Data;Internet;Data mining;area studies;big data;machine learning;Named Entity Recognition;Geocoding;LDA;KyTea;BiLSTM-CRF},
  doi={10.23919/PNC53575.2021.9672311}
}

@inproceedings{kameko2020annotating,
    title = "Annotating Event Appearance for {J}apanese Chess Commentary Corpus",
    author = "Kameko, Hirotaka  and
      Mori, Shinsuke",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = 2020,
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.530",
    pages = "4302--4308",
    abstract = "In recent years, there has been a surge of interest in natural language processing related to the real world, such as symbol grounding, language generation, and non-linguistic data search by natural language queries. Researchers usually collect pairs of text and non-text data for research. However, the text and non-text data are not always a {``}true{''} pair. We focused on the shogi (Japanese chess) commentaries, which are accompanied by game states as a well-defined {``}real world{''}. For analyzing and processing texts accurately, considering only the given states is insufficient, and we must consider the relationship between texts and the real world. In this paper, we propose {``}Event Appearance{''} labels that show the relationship between events mentioned in texts and those happening in the real world. Our event appearance label set consists of temporal relation, appearance probability, and evidence of the event. Statistics of the annotated corpus and the experimental result show that there exists temporal relation which skillful annotators realize in common. However, it is hard to predict the relationship only by considering the given states.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{matsuyoshi2018annotating,
    title = "Annotating Modality Expressions and Event Factuality for a {J}apanese Chess Commentary Corpus",
    author = "Matsuyoshi, Suguru  and
      Kameko, Hirotaka  and
      Murawaki, Yugo  and
      Mori, Shinsuke",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = 2018,
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1393",
}

@inproceedings{kameko2018deep,
  author="Kameko, Hirotaka
    and Suzuki, Jun
    and Mizukami, Naoki
    and Tsuruoka, Yoshimasa",
  editor="Cazenave, Tristan
    and Winands, Mark H.M.
    and Saffidine, Abdallah",
  title="Deep Reinforcement Learning with Hidden Layers on Future States",
  booktitle="Computer Games",
  year=2018,
  publisher="Springer International Publishing",
  address="Cham",
  pages="46--60",
  abstract="Deep reinforcement learning algorithms such as Deep Q-Networks have successfully been used to construct a strong agent for Atari games by only performing direct evaluation of the current state and actions. This is in stark contrast to the algorithms for traditional board games such as Chess or Go, where a look-ahead search mechanism is indispensable to build a strong agent. In this paper, we present a novel deep reinforcement learning architecture that can both effectively and efficiently use information on future states in video games. First, we demonstrate that such information is indeed quite useful in deep reinforcement learning by using exact state transition information obtained from the emulator. We then propose a method that predicts future states using Long Short Term Memory (LSTM), such that the agent can look ahead without the emulator. In this work, we applied our method to the asynchronous advantage actor-critic (A3C) architecture. The experimental results show that our proposed method with predicted future states substantially outperforms the vanilla A3C in several Atari games.",
  isbn="978-3-319-75931-9"
}

@inproceedings{mizukami2017exploration,
  author="Mizukami, Naoki
    and Suzuki, Jun
    and Kameko, Hirotaka
    and Tsuruoka, Yoshimasa",
  editor="Winands, Mark H.M. 
    and van den Herik, H. Jaap
    and Kosters, Walter A.",
  title="Exploration Bonuses Based on Upper Confidence Bounds for Sparse Reward Games",
  booktitle="Advances in Computer Games",
  year=2017,
  publisher="Springer International Publishing",
  address="Cham",
  pages="165--175",
  abstract="Recent deep reinforcement learning (RL) algorithms have achieved super-human-level performance in many Atari games. However, a closer look at their performance reveals that the algorithms fall short of humans in games where rewards are only obtained occasionally. One solution to this sparse reward problem is to incorporate an explicit and more sophisticated exploration strategy in the agent's learning process. In this paper, we present an effective exploration strategy that explicitly considers the progress of training using exploration bonuses based on Upper Confidence Bounds (UCB). Our method also includes a mechanism to separate exploration bonuses from rewards, thereby avoiding the problem of interfering with the original learning objective. We evaluate our method on Atari 2600 games with sparse rewards, and achieve significant improvements over the vanilla asynchronous advantage actor-critic (A3C) algorithm.",
  isbn="978-3-319-71649-7"
}

@inproceedings{ushiku2017game,
  author = {Ushiku, Atsushi and Mori, Shinsuke and Kameko, Hirotaka and Tsuruoka, Yoshimasa},
  title = {Game State Retrieval with Keyword Queries},
  year = 2017,
  isbn = {9781450350228},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3077136.3080668},
  doi = {10.1145/3077136.3080668},
  abstract = {There are many databases of game records available online. In order to retrieve a game state from such a database, users usually need to specify the target state in a domain-specific language, which may be difficult to learn for novice users. In this work, we propose a search system that allows users to retrieve game states from a game record database by using keywords. In our approach, we first train a neural network model for symbol grounding using a small number of pairs of a game state and a commentary on it. We then apply it to all the states in the database to associate each of them with characteristic terms and their scores. The enhanced database thus enables users to search for a state using keywords. To evaluate the performance of the proposed method, we conducted experiments of game state retrieval using game records of Shogi (Japanese chess) with commentaries. The results demonstrate that our approach gives significantly better results than full-text search and an LSTM language model.},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {877–880},
  numpages = {4},
  keywords = {symbol grounding, shogi, search of nonlinguistic data},
  location = {Shinjuku, Tokyo, Japan},
  series = {SIGIR '17}
}

@inproceedings{mori2016japanese,
    title = "A {J}apanese Chess Commentary Corpus",
    author = "Mori, Shinsuke  and
      Richardson, John  and
      Ushiku, Atsushi  and
      Sasada, Tetsuro  and
      Kameko, Hirotaka  and
      Tsuruoka, Yoshimasa",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = 2016,
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1225",
    pages = "1415--1420",
    abstract = "In recent years there has been a surge of interest in the natural language prosessing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries. In order to concentrate on language ambiguities, we propose to use a well-defined {``}real world,{''} that is game states. We built a corpus consisting of pairs of sentences and a game state. The game we focus on is shogi (Japanese chess). We collected 742,286 commentary sentences in Japanese. They are spontaneously generated contrary to natural language annotations in many image datasets provided by human workers on Amazon Mechanical Turk. We defined domain specific named entities and we segmented 2,508 sentences into words manually and annotated each word with a named entity tag. We describe a detailed definition of named entities and show some statistics of our game commentary corpus. We also show the results of the experiments of word segmentation and named entity recognition. The accuracies are as high as those on general domain texts indicating that we are ready to tackle various new problems related to the real world.",
}

@inproceedings{kameko2015learning,
  author={Kameko, Hirotaka and Mori, Shinsuke and Tsuruoka, Yoshimasa},
  booktitle={2015 IEEE Conference on Computational Intelligence and Games (CIG)}, 
  title={Learning a game commentary generator with grounded move expressions}, 
  year=2015,
  volume={},
  number={},
  pages={177-184},
  keywords={Games;Natural languages;Computers;Training;Grounding;Law},
  doi={10.1109/CIG.2015.7317930}
}

@inproceedings{kameko2015symbol,
    title = "Can Symbol Grounding Improve Low-Level {NLP}? Word Segmentation as a Case Study",
    author = "Kameko, Hirotaka  and
      Mori, Shinsuke  and
      Tsuruoka, Yoshimasa",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = 2015,
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1277",
    doi = "10.18653/v1/D15-1277",
    pages = "2298--2303",
}

@misc{kubo2024annotation,
 author = {久保 旭 and 亀甲 博貴 and 原島 純 and 木村 明日香 and 後藤 真 and 関野 樹 and 森 信介},
 book = {じんもんこん2024論文集},
 month=nov,
 note = {時間表現の推定は時間表現の認識タスクと正規化タスクに分けられ，総称して TERN
（Temporal Entity Recognition and Normalization）と呼ばれる．これらの自動推定に当たっては時間表現のデータセット構築が重要となるが，広く使用されているアノテーション基準には，検索や可視化が困難な曖昧な表現が残るという課題がある。本研究では，主観的な時間表現を時間軸上にプロット可能にする新たなアノテーション基準を提案し，これに基づいて時間表現のタグ推定，絶対値推定，可視化を行う．, Temporal entity recognition and normalization (TERN) is a task for estimating temporal expressions. In automatic estimation of these tasks, it is important to construct a dataset of temporal expressions. However, widely used annotation criteria have the problem that some ambiguous expressions remain, which are difficult to search and visualize. In this research, we propose a new annotation standard that allows subjective temporal expressions to be plotted on a time axis. In addition, we also perform tag prediction, absolute value estimation, and visualization of temporal expressions based on this standard.},
 pages = {125--132},
 publisher = {情報処理学会},
 title = {主観的な時間表現のアノテーションと可視化},
 volume = {2024},
 year = {2024},
 url={https://ipsj.ixsq.nii.ac.jp/records/241519},
}

@misc{kubo2024time,
  author={久保 旭 and 亀甲 博貴 and 原島 純 and 木村 明日香 and 関野 樹 and 森 信介},
  title={主観的な時間表現を含む時間区間アノテーションデータセットの構築},
  howpublished={第261回情報処理学会言語処理研究会 (NL-261)},
  year=2024,
  month=aug,
  url={http://id.nii.ac.jp/1001/00238382/},
}

@misc{kajimura2024multimodal,
  author={梶村 恵矢 and 西村 太一 and 羽路 悠斗 and 山本 航輝 and 崔 泰毓 and 亀甲 博貴 and 森 信介},
  title={一人称視点映像を用いたマルチモーダル作業支援システム},
  howpublished={言語処理学会第30回年次大会 (NLP 2024)},
  month = mar,
  year=2024,
}

@misc{haneji2024egooops,
  author={羽路 悠斗 and 西村 太一 and 山本 航輝 and 梶村 恵矢 and 崔 泰毓 and 亀甲 博貴 and 森 信介},
  title={EgoOops!データセット：手順書に従う作業の一人称視点映像への作業誤りアノテーション},
  howpublished={言語処理学会第30回年次大会 (NLP 2024)},
  month=mar,
  year=2024,
}

@misc{nakata2024recognizing,
  author={仲田 将斗 and 亀甲 博貴 and 森 信介},
  title={テキストアナリティクスツールの説明文に含まれる設定キーの認識},
  howpublished={言語処理学会第30回年次大会 (NLP 2024)},
  month=mar,
  year=2024,
}

@misc{kajimura2023multimodal,
  author={梶村 恵矢 and 西村 太一 and 羽路 悠斗 and 山本 航輝 and 崔 泰毓 and 亀甲 博貴 and 森 信介},
  title={一人称視点映像を用いたマルチモーダル作業支援システムの提案},
  howpublished={第31回インタラクティブシステムとソフトウェアに関するワークショップ (WISS 2023)},
  month=dec,
  year=2023,
}

@misc{yoshida2023image,
  author={吉田 智哉 and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={単語の階層関係に基づくデータ拡張を利用した画像キャプション生成の検討},
  howpublished={言語処理学会第29回年次大会 (NLP 2023)},
  month=mar,
  year=2023,
}

@misc{yamamoto2023protocol,
  author={山本 航輝 and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={VideoClipを用いた実験動画からのプロトコル生成},
  howpublished={言語処理学会第29回年次大会 (NLP 2023)},
  month=mar,
  year=2023,
}

@misc{kinoshita2023news,
  author={木下 聖 and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={株式市場の出来事の長期的視野での理解を支援するニュース記事抽出によるストーリー可視化},
  howpublished={言語処理学会第29回年次大会 (NLP 2023)},
  month=mar,
  year=2023,
}

@misc{ohno2023geocoding,
  author={大野 けやき and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={テキスト中の場所表現認識と係り受けに基づく緯度経度推定ツールの開発},
  howpublished={言語処理学会第29回年次大会 (NLP 2023)},
  month=mar,
  year=2023,
}

@misc{morita2023description,
  author={森田 康介 and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={テキストアナリティクスツールの操作ログからの実験設定の説明文生成},
  howpublished={言語処理学会第29回年次大会 (NLP 2023)},
  month=mar,
  year=2023,
}

@misc{shirai2023open,
  author={白井 圭佑 and 亀甲 博貴 and 森 信介},
  title={オープンドメインの手順書のフローグラフ予測とデータセットの構築},
  howpublished={言語処理学会第29回年次大会 (NLP 2023)},
  month=mar,
  year=2023,
}

@misc{morita2022description,
  author={森田 康介 and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={テキストマイニングツールのログからの実験設定の説明文生成},
  year=2022,
  howpublished={第253回情報処理学会言語処理研究会 (NL-253)},
  month=sep,
  url={http://id.nii.ac.jp/1001/00220128/},
}

@misc{shirai2022recipe,
  author={白井 圭佑 and 橋本 敦史 and 牛久 祥孝 and 栗田 修平 and 亀甲 博貴 and 森 信介},
  title={レシピ分野における動作対象の状態変化を考慮したデータセットの構築と検索モデルの提案},
  howpublished={言語処理学会第28回年次大会 (NLP 2022)},
  month=mar,
  year=2022,
  url={https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/G1-1.pdf},
}

@misc{tanaka2022error,
  author={田中 健斗 and 西村 太一 and 南條 浩輝 and 白井 圭佑 and 亀甲 博貴},
  title={画像描写問題における学習者作文の誤り訂正},
  howpublished={言語処理学会第28回年次大会 (NLP 2022)},
  month=mar,
  year=2022,
  url={https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/E4-3.pdf},
}

@misc{hoshijima2022citizen,
  author={星島 洸明 and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={市民科学でのアノテーション作業支援と作業者の能力向上支援},
  howpublished={言語処理学会第28回年次大会 (NLP 2022)},
  month=mar,
  year=2022,
  url={https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/PH1-3.pdf},
}

@misc{tanaka2021automatic,
  author={田中 健斗 and 西村 太一 and 白井 圭佑 and 亀甲 博貴 and 森 信介},
  title={写真描画問題の自動採点手法の検討},
  howpublished={人工知能学会第35回全国大会 (JSAI 2021)},
  month=jun,
  year=2021,
  doi={10.11517/pjsai.JSAI2021.0_4J2GS6e03},
}

@misc{hoshijima2021annotation,
  author={星島 洸明 and 西村 太一 and 亀甲 博貴 and 森 信介},
  title={複数作業者を想定したアノテーションツールの作成と機能の検討},
  howpublished={言語処理学会第27回年次大会 (NLP 2021)},
  month=mar,
  year=2021,
  url={https://anlp.jp/proceedings/annual_meeting/2021/pdf_dir/P2-13.pdf},
}

@misc{nishimura2021procedure,
  author={西村 太一 and 橋本 敦史 and 牛久 祥孝 and 亀甲 博貴 and 森 信介},
  title={手順構造を考慮した作業映像からの手順書生成},
  howpublished={言語処理学会第27回年次大会 (NLP 2021)},
  month=mar,
  year=2021,
  url={https://anlp.jp/proceedings/annual_meeting/2021/pdf_dir/P5-13.pdf},
}

@misc{kawabata2020image,
  author={川端 公貴 and 南條 浩輝 and 亀甲 博貴 and 森 信介},
  title={画像キャプションを用いた日本語学習支援の検討},
  howpublished={言語処理学会第26回年次大会 (NLP 2020)},
  month=mar,
  year=2020,
  url={https://anlp.jp/proceedings/annual_meeting/2020/pdf_dir/F2-2.pdf},
}

@misc{tomori2020joint,
  author={友利 涼 and 亀甲 博貴 and 森 信介},
  title={テキストと非テキストデータからの同時事前学習},
  howpublished={言語処理学会第26回年次大会 (NLP 2020)},
  month=mar,
  year=2020,
  url={https://anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P3-31.pdf},
}

@misc{ogawa2020wikitext,
  author={小川 晃 and 亀甲 博貴 and 森 信介},
  title={WikiText-JA構築によるBERT事前学習の効率化},
  howpublished={言語処理学会第26回年次大会 (NLP 2020)},
  month=mar,
  year=2020,
  url={https://anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P5-1.pdf},
}

@misc{kameko2020event,
  author={亀甲 博貴 and 森 信介},
  title={熟練者による解説文内イベントの出現とその根拠のアノテーション},
  howpublished={言語処理学会第26回年次大会 (NLP 2020)},
  month=mar,
  year=2020,
  url={https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P5-33.pdf},
}

@misc{tomori2019modality,
  author={友利 涼 and 村脇 有吾 and 松吉 俊 and 亀甲 博貴 and 森 信介},
  title={モダリティ表現認識・事象の事実性解析の同時学習},
  howpublished={第241回情報処理学会自然言語処理研究会 (NL-241)},
  month=aug,
  year=2019,
}

@misc{kameko2019monte,
  author={亀甲 博貴 and 松吉 俊 and 村脇 有吾 and 森 信介},
  title={モンテカルロシミュレーションによる認識的モダリティ表現のグラウンディング手法の検討},
  howpublished={言語処理学会第25回年次大会 (NLP 2019)},
  month=mar,
  year=2019,
  url={https://anlp.jp/proceedings/annual_meeting/2019/pdf_dir/A4-3.pdf},
}

@misc{matsuyoshi2017modality,
  author={松吉 俊 and 村脇 有吾 and 亀甲 博貴 and 森 信介},
  title={将棋解説文へのモダリティ情報アノテーション},
  howpublished={第233回情報処理学会言語処理研究会 (NL-233)},
  month=oct,
  year=2017,
  url={http://id.nii.ac.jp/1001/00183700/},
}

@misc{kameko2016realization,
  author={亀甲 博貴 and 森 信介 and 鶴岡 慶雅},
  title={実現確率に基づく解説すべき指し手の推定},
  howpublished={第21回ゲームプログラミングワークショップ (GPW 2016)},
  month=nov,
  year=2016,
  url={http://id.nii.ac.jp/1001/00175300/},
}

@misc{ushiku2016game,
  author={牛久 敦 and 森 信介 and 亀甲 博貴 and 鶴岡 慶雅},
  title={特徴語との自動対応によるゲーム局面の検索},
  howpublished={第8回データ工学と情報マネジメントに関するフォーラム (DEIM 2016)},
  month=mar,
  year=2016,
  url={https://db-event.jpn.org/deim2016/papers/54.pdf},
}

@misc{kameko2014grounding,
  author={亀甲 博貴 and 森 信介 and 鶴岡 慶雅},
  title={将棋解説文のグラウンディングのための指し手表現と局面状態の対応付け},
  howpublished={第19回ゲームプログラミングワークショップ (GPW 2014)},
  month=nov,
  year=2014,
  url={http://id.nii.ac.jp/1001/00106499/},
  memo={優秀論文賞受賞},
}

@misc{kameko2014logistic,
  author={亀甲 博貴 and 三輪 誠 and 鶴岡 慶雅 and 森 信介 and 近山 隆},
  title={ロジスティック回帰による言語モデルを用いた将棋解説文の自動生成},
  howpublished={言語処理学会第20回年次大会 (NLP 2014)},
  month=mar,
  year=2014,
  url={https://anlp.jp/proceedings/annual_meeting/2014/pdf_dir/P8-18.pdf},
}

@misc{kameko2013shogi,
  author={亀甲 博貴 and 浦 晃 and 三輪 誠 and 鶴岡 慶雅 and 森 信介 and 近山 隆},
  title={将棋解説の自動解説のための局面からの特徴語生成},
  howpublished={第18回ゲームプログラミングワークショップ (GPW 2013)},
  month=nov,
  year=2013,
  url={http://id.nii.ac.jp/1001/00095787/},
  memo={研究奨励賞受賞},
}

@misc{kameko2012probcut,
  author={亀甲 博貴 and 浦 晃 and 三輪 誠 and 鶴岡 慶雅 and 近山 隆},
  title={局面情報からの探索信頼性の推定による将棋のProbCutの性能向上},
  howpublished={第17回ゲームプログラミングワークショップ (GPW 2012)},
  month=nov,
  year=2012,
  url={http://id.nii.ac.jp/1001/00091332/},
}

@misc{kameko2024attention,
  author={亀甲 博貴 and 白井 圭佑 and 森 信介},
  title={基礎から一歩進んだ深層学習トピックス—III—注意機構と大規模言語モデル},
  howpublished={システム制御情報学会誌「システム／制御／情報」},
  volume={68},
  number={6},
  month=jun,
  year=2024,
},

@misc{kameko2019shogi,
  author={亀甲 博貴 and 森 信介},
  title={将棋棋譜解説の自動生成（特集：「深層学習による言語生成」）},
  howpublished={人工知能学会誌「人工知能」},
  volume={34},
  month=jul,
  year=2019,
  url={http://id.nii.ac.jp/1004/00010208/},
}
